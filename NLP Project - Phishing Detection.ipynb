{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "import re\n",
    "import contractions\n",
    "import random\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix, ConfusionMatrixDisplay, precision_recall_fscore_support as score\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import warnings\n",
    "import gensim.downloader as api\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('omw-1.4')\n",
    "stopWords=set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('./Dataset/fraud_email_.csv')\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset[dataset['Class'] == 1]), len(dataset[dataset['Class'] == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_contractions(text):\n",
    "    words = []\n",
    "    for word in text.split():  \n",
    "        words.append(contractions.fix(word))\n",
    "    res = ' '.join(words)\n",
    "    return res\n",
    "\n",
    "def dataCleaning(text):\n",
    "    text=str(text)\n",
    "    text=text.lower()\n",
    "    # Remove non-alphabetical characters and extra spaces\n",
    "    text = remove_contractions(text)\n",
    "    text = re.sub(r'\\.', ' . ', text)\n",
    "    text = re.sub(r'\\,', ' , ', text)\n",
    "    text = re.sub(r'\\:', ' : ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    words=[]\n",
    "     # Perform contractions\n",
    "    return text\n",
    "\n",
    "def removestopwords(text):\n",
    "    words=text.split(' ')\n",
    "    words = [word for word in words if word not in stopWords]    \n",
    "    text = ' '.join(words)\n",
    "    return text\n",
    "\n",
    "def performlemmatization(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    words=text.split(' ')\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]    \n",
    "    text = ' '.join(words)\n",
    "    return text\n",
    "\n",
    "def add_space_for_punctuations(text):\n",
    "    # This regular expression captures:\n",
    "    # 1. A word (\\w+)\n",
    "    # 2. A punctuation character ([.,!?])\n",
    "    pattern = re.compile(r'(\\w+)([.,!?])')\n",
    "    result = pattern.sub(r'\\1 \\2', text)\n",
    "    return result\n",
    "\n",
    "def remove_tags(html):\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    for data in soup(['style', 'script']):\n",
    "        data.decompose()\n",
    "    return ' '.join(soup.stripped_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_data(data, column_name):\n",
    "    data[\"email_body_clean\"] = data[column_name].apply(dataCleaning)\n",
    "    data[\"email_body_clean\"] = data[\"email_body_clean\"].apply(removestopwords)\n",
    "    data['email_body_clean'] =  [re.sub('[^a-zA-Z0-9]', ' ', str(x)) for x in data['email_body_clean']]\n",
    "    data['email_body_clean'] =  [re.sub(' +', ' ', str(x)) for x in data['email_body_clean']]\n",
    "    # data[\"email_body_clean\"] = data[\"email_body_clean\"].apply(add_space_for_punctuations)\n",
    "    # data[\"email_body_clean\"] = data[\"email_body_clean\"].apply(performlemmatization)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pre_process_data(dataset, \"Text\")\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run models using TFIDF as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_features = tfidf_vectorizer.fit_transform(dataset[\"email_body_clean\"])\n",
    "X_train_tfidf, X_test_tfidf, y_train, y_test = train_test_split(tfidf_features, dataset['Class'].to_numpy(), \n",
    "                                                                test_size=0.2, random_state=SEED)\n",
    "\n",
    "count_vectorizer = CountVectorizer()\n",
    "count_features = count_vectorizer.fit_transform(dataset[\"email_body_clean\"])\n",
    "\n",
    "X_train_count, X_test_count, _, _ = train_test_split(count_features, dataset['Class'].to_numpy(), \n",
    "                                                                test_size=0.2, random_state=SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_scores(y_test, y_predtest, print_matrix = True):\n",
    "    precision = precision_score(y_test, y_predtest,average=\"macro\")\n",
    "    recall = recall_score(y_test, y_predtest,average=\"macro\")\n",
    "    f1 = f1_score(y_test, y_predtest,average=\"macro\")\n",
    "    accuracy = accuracy_score(y_test, y_predtest)\n",
    "    print(precision,recall,f1,accuracy)\n",
    "    \n",
    "    cm = confusion_matrix(y_test, y_predtest)\n",
    "    TN1, FP1, FN1, TP1 = cm[0][0], cm[0][1], cm[1][0], cm[1][1]\n",
    "    print(\"precision for 1 - \", TP1/(TP1+FP1), \"Recall for 1 - \",TP1/(TP1+FN1))\n",
    "    #print(TP+FN,sum(y_test))\n",
    "    y_test_inv = [ abs(i-1) for i in y_test]\n",
    "    y_predtest_inv = [ abs(i-1) for i in y_predtest]\n",
    "    \n",
    "    cm = confusion_matrix(y_test_inv, y_predtest_inv)\n",
    "    TN0, FP0, FN0, TP0 = cm[0][0], cm[0][1], cm[1][0], cm[1][1]\n",
    "    #print(TP+FN,sum(y_test))\n",
    "    print(\"precision for 0 - \", TP0/(TP0+FP0), \"Recall for 0 - \",TP0/(TP0+FN0))\n",
    "    \n",
    "    if(print_matrix):\n",
    "        confusion_mat = confusion_matrix(y_test, y_predtest, labels=[0,1])\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=confusion_mat, display_labels=[0,1])\n",
    "        disp.plot()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(y_test), len(y_test), sum(y_train), len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfidf.shape, X_test_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_tfidf = LinearSVC(C=1.0)\n",
    "SVM_tfidf.fit(X_train_tfidf, y_train)\n",
    "ypredtest_svm_tfidf = SVM_tfidf.predict(X_test_tfidf)\n",
    "\n",
    "SVM_count = LinearSVC(C=1.0)\n",
    "SVM_count.fit(X_train_count, y_train)\n",
    "ypredtest_svm_count = SVM_count.predict(X_test_count)\n",
    "\n",
    "perceptron_tfidf = Perceptron()\n",
    "perceptron_tfidf.fit(X_train_count, y_train)\n",
    "ypredtest_perceptron = perceptron_tfidf.predict(X_test_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"SVM with TFIDF:\")\n",
    "print_scores(y_test, ypredtest_svm_tfidf)\n",
    "\n",
    "print(\"SVM with count:\")\n",
    "print_scores(y_test, ypredtest_svm_count)\n",
    "\n",
    "print(\"Perceptron with count:\")\n",
    "print_scores(y_test, ypredtest_perceptron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## TEST The Model with Other datasets ##########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iwspa_dataset = pd.read_csv('./Dataset/IWSPA-AP-Parsed.csv')\n",
    "iwspa_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iwspa_dataset = pre_process_data(iwspa_dataset, \"email_body\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iwspa_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_features_iwspa = tfidf_vectorizer.transform(iwspa_dataset[\"email_body_clean\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testLabels = iwspa_dataset['is_phishing'].to_numpy()\n",
    "ypred_iwspa_svm_tfidf = SVM_tfidf.predict(tfidf_features_iwspa)\n",
    "ypred_iwspa_svm_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"SVM with TFIDF Test:\")\n",
    "print_scores(testLabels, ypred_iwspa_svm_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del X_train_tfidf, X_test_tfidf, X_train_count, X_test_count, tfidf_vectorizer\n",
    "# del tfidf_features, count_vectorizer, count_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Word2Vec features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word2vec_features(data):\n",
    "    master=[]\n",
    "    for index, row in data.iterrows():\n",
    "        sentence = row['email_body_clean']\n",
    "        words=sentence.split(' ')\n",
    "        sentence_vector=[]\n",
    "        for word in words:\n",
    "            sentence_vector.append(wv[word] if word in wv else np.zeros(300))\n",
    "        master.append(np.mean(sentence_vector, axis=0))\n",
    "    return np.array(master)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_word2vec_features = get_word2vec_features(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train_word2vec_features, dataset['Class'].to_numpy(), \n",
    "                                                    test_size=0.2, random_state=SEED)\n",
    "# del train_word2vec_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM model\n",
    "print(\"SVM with Word2Vec Train (train/val/test) :\")\n",
    "svm_model = LinearSVC(C=1.0)\n",
    "svm_model.fit(X_train, y_train)\n",
    "y_pred_svm = svm_model.predict(X_test)\n",
    "print_scores(y_test, y_pred_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test features\n",
    "print(\"SVM with Word2Vec Test:\")\n",
    "test_features_iwspa = get_word2vec_features(iwspa_dataset)\n",
    "testLabels = iwspa_dataset['is_phishing'].to_numpy()\n",
    "ypred_iwspa_svm = svm_model.predict(test_features_iwspa)\n",
    "print_scores(testLabels, ypred_iwspa_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, dataloader):\n",
    "    prediction_list = []\n",
    "    for data, target in dataloader:\n",
    "        outputs = model(data)\n",
    "        _, predicted = torch.max(outputs.data, 1) \n",
    "        prediction_list.extend(list(np.array(predicted.cpu())))\n",
    "    return prediction_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Max sentence length :\", max([len(x) for x in dataset['email_body_clean'].str.split()]))\n",
    "print(\"Average sentence length :\", sum([len(x) for x in dataset['email_body_clean'].str.split()]) / dataset['email_body_clean'].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleGRU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleGRU, self).__init__()\n",
    "        self.GRU = nn.GRU(300, 100, batch_first=True)\n",
    "        self.fc = nn.Linear(100, 2)\n",
    "        #self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.GRU(x)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "    \n",
    "# initialize the NN\n",
    "simple_gru_model = SimpleGRU()\n",
    "print(simple_gru_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        feature = torch.tensor(self.features[index], dtype=torch.float32)\n",
    "        label = torch.tensor(self.labels[index], dtype=torch.long) \n",
    "        return feature, label\n",
    "\n",
    "def get_rnn_features(data):\n",
    "    rnn_features_vector = []\n",
    "    for index, row in data.iterrows():\n",
    "        sentence = row['email_body_clean']\n",
    "        words=sentence.split(' ')\n",
    "        sentence_vector=[]\n",
    "        for word in words[:100]:\n",
    "            sentence_vector.append(wv[word] if word in wv else np.zeros(300))\n",
    "        if len(sentence_vector)<100:\n",
    "            for i in range(100-len(sentence_vector)):\n",
    "                sentence_vector.append(np.zeros(300))\n",
    "        rnn_features_vector.append(sentence_vector)\n",
    "    return np.array(rnn_features_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rnn_features = get_rnn_features(dataset)\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_rnn_features, dataset['Class'].to_numpy(), \n",
    "                                                    test_size=0.2, random_state=SEED)\n",
    "train_data = CustomDataset(features=X_train, labels=np.array(y_train))\n",
    "test_data=CustomDataset(features=X_test, labels=np.array(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del train_rnn_features, X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 0\n",
    "# the number of samples per batch to load\n",
    "batch_size = 20\n",
    "# percentage of training set to use as validation\n",
    "valid_size = 0.2\n",
    "\n",
    "num_train = len(train_data)\n",
    "indices = list(range(num_train))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(valid_size * num_train))\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
    "    sampler=train_sampler, num_workers=num_workers)\n",
    "valid_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, \n",
    "    sampler=valid_sampler, num_workers=num_workers)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, \n",
    "    num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test features\n",
    "test_rnn_features = get_rnn_features(iwspa_dataset)\n",
    "testLabels = iwspa_dataset['is_phishing'].to_numpy()\n",
    "test_iwspa_loader = torch.utils.data.DataLoader(CustomDataset(features=test_rnn_features, labels=testLabels), \n",
    "                                                batch_size=batch_size, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, n_epochs, train_loader, valid_loader, \n",
    "                test_loader, test_labels, new_test_loader, new_test_labels):\n",
    "    best_model = None\n",
    "    valid_loss_min = np.Inf\n",
    "    for epoch in range(n_epochs):\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "        model.train()\n",
    "        print(\"Starting epoch\", epoch+1)\n",
    "        batch_no = 1\n",
    "        for data, target in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()*data.size(0)\n",
    "            print(\"Finished batch\", batch_no)\n",
    "            batch_no += 1\n",
    "\n",
    "        # validate the model\n",
    "        model.eval()\n",
    "        for data, target in valid_loader:\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            valid_loss += loss.item()*data.size(0)\n",
    "\n",
    "        train_loss = train_loss/len(train_loader.dataset)\n",
    "        valid_loss = valid_loss/len(valid_loader.dataset)\n",
    "\n",
    "        ypred_test = predict(model, test_loader)\n",
    "        print(\"Accuracy scores on validation data :\")\n",
    "        print_scores(test_labels, ypred_test, print_matrix = False)\n",
    "\n",
    "        ypred_new_test = predict(model, new_test_loader)\n",
    "        print(\"Accuracy scores on iwspa data :\")\n",
    "        print_scores(new_test_labels, ypred_new_test, print_matrix = False)\n",
    "\n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "            epoch+1, \n",
    "            train_loss,\n",
    "            valid_loss,\n",
    "            ))\n",
    "\n",
    "        # saving the model if validation loss has decreased\n",
    "        if valid_loss <= valid_loss_min:\n",
    "            print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "            valid_loss_min,\n",
    "            valid_loss))\n",
    "            torch.save(model.state_dict(), 'model.pt')\n",
    "            valid_loss_min = valid_loss\n",
    "            best_model = model\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "N_EPOCHS = 50\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(simple_gru_model.parameters())\n",
    "\n",
    "best_model = train_model(simple_gru_model, criterion, optimizer, N_EPOCHS, train_loader, \n",
    "                         valid_loader, test_loader, y_test, test_iwspa_loader, testLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load('model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
